created virtual environment CPython3.11.5.final.0-64 in 904ms
  creator CPython3Posix(dest=/localscratch/alexort.47178032.0/env, clear=False, no_vcs_ignore=False, global=False)
  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/home/alexort/.local/share/virtualenv)
    added seed packages: pip==23.2.1, setuptools==68.0.0, wheel==0.41.1
  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator
Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/einops-0.8.1+computecanada-py3-none-any.whl (from -r requirements.txt (line 1))
Requirement already satisfied: numpy==2.2.2+computecanada in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2.2.2+computecanada)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/PyYAML-6.0.2+computecanada-cp311-cp311-linux_x86_64.whl (from -r requirements.txt (line 3))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/scikit_learn-1.6.1+computecanada-cp311-cp311-linux_x86_64.whl (from -r requirements.txt (line 4))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/torch-2.7.1+computecanada-cp311-cp311-linux_x86_64.whl (from -r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/tqdm-4.67.1+computecanada-py3-none-any.whl (from -r requirements.txt (line 6))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/wandb-0.19.6+computecanada-py3-none-linux_x86_64.whl (from -r requirements.txt (line 7))
Requirement already satisfied: scipy>=1.6.0 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from scikit_learn==1.6.1+computecanada->-r requirements.txt (line 4)) (1.15.1+computecanada)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/joblib-1.5.1+computecanada-py3-none-any.whl (from scikit_learn==1.6.1+computecanada->-r requirements.txt (line 4))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/threadpoolctl-3.6.0+computecanada-py3-none-any.whl (from scikit_learn==1.6.1+computecanada->-r requirements.txt (line 4))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/filelock-3.18.0+computecanada-py3-none-any.whl (from torch==2.7.1->-r requirements.txt (line 5))
Requirement already satisfied: typing-extensions>=4.10.0 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/ipykernel/2025a/lib/python3.11/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (4.12.2+computecanada)
Requirement already satisfied: sympy>=1.13.3 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (1.13.3+computecanada)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/networkx-3.5+computecanada-py3-none-any.whl (from torch==2.7.1->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/jinja2-3.1.6+computecanada-py3-none-any.whl (from torch==2.7.1->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/fsspec-2025.7.0+computecanada-py3-none-any.whl (from torch==2.7.1->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/click-8.2.1+computecanada-py3-none-any.whl (from wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/docker_pycreds-0.4.0+computecanada-py2.py3-none-any.whl (from wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/gitpython-3.1.45+computecanada-py3-none-any.whl (from wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Requirement already satisfied: platformdirs in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/ipykernel/2025a/lib/python3.11/site-packages (from wandb==0.19.6+computecanada->-r requirements.txt (line 7)) (3.10.0+computecanada)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/x86-64-v3/protobuf-5.29.5+computecanada-cp311-cp311-linux_x86_64.whl (from wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Requirement already satisfied: psutil>=5.0.0 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/ipykernel/2025a/lib/python3.11/site-packages (from wandb==0.19.6+computecanada->-r requirements.txt (line 7)) (6.1.1+computecanada)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/pydantic-2.11.7+computecanada-py3-none-any.whl (from wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/requests-2.32.4+computecanada-py3-none-any.whl (from wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/sentry_sdk-2.33.2+computecanada-py2.py3-none-any.whl (from wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/setproctitle-1.3.4+computecanada-cp311-cp311-linux_x86_64.whl (from wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Requirement already satisfied: setuptools in /localscratch/alexort.47178032.0/env/lib/python3.11/site-packages (from wandb==0.19.6+computecanada->-r requirements.txt (line 7)) (68.0.0)
Requirement already satisfied: six>=1.4.0 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb==0.19.6+computecanada->-r requirements.txt (line 7)) (1.17.0+computecanada)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/gitdb-4.0.12+computecanada-py3-none-any.whl (from gitpython!=3.1.29,>=1.0.0->wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/annotated_types-0.7.0+computecanada-py3-none-any.whl (from pydantic<3,>=2.6->wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/gentoo2023/generic/pydantic_core-2.33.2+computecanada-cp311-cp311-linux_x86_64.whl (from pydantic<3,>=2.6->wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/typing_inspection-0.4.1+computecanada-py3-none-any.whl (from pydantic<3,>=2.6->wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/charset_normalizer-3.4.2+computecanada-py3-none-any.whl (from requests<3,>=2.0.0->wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/idna-3.10+computecanada-py3-none-any.whl (from requests<3,>=2.0.0->wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/urllib3-2.5.0+computecanada-py3-none-any.whl (from requests<3,>=2.0.0->wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/certifi-2025.7.14+computecanada-py3-none-any.whl (from requests<3,>=2.0.0->wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/scipy-stack/2025a/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.7.1->-r requirements.txt (line 5)) (1.3.0+computecanada)
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/MarkupSafe-2.1.5+computecanada-cp311-cp311-linux_x86_64.whl (from jinja2->torch==2.7.1->-r requirements.txt (line 5))
Processing /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic/smmap-5.0.2+computecanada-py3-none-any.whl (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.19.6+computecanada->-r requirements.txt (line 7))
Installing collected packages: urllib3, typing-inspection, tqdm, threadpoolctl, smmap, setproctitle, PyYAML, pydantic-core, protobuf, networkx, MarkupSafe, joblib, idna, fsspec, filelock, einops, docker-pycreds, click, charset-normalizer, certifi, annotated-types, sentry-sdk, scikit_learn, requests, pydantic, jinja2, gitdb, torch, gitpython, wandb
Successfully installed MarkupSafe-2.1.5+computecanada PyYAML-6.0.2+computecanada annotated-types-0.7.0+computecanada certifi-2025.7.14+computecanada charset-normalizer-3.4.2+computecanada click-8.2.1+computecanada docker-pycreds-0.4.0+computecanada einops-0.8.1+computecanada filelock-3.18.0+computecanada fsspec-2025.7.0+computecanada gitdb-4.0.12+computecanada gitpython-3.1.45+computecanada idna-3.10+computecanada jinja2-3.1.6+computecanada joblib-1.5.1+computecanada networkx-3.5+computecanada protobuf-5.29.5+computecanada pydantic-2.11.7+computecanada pydantic-core-2.33.2+computecanada requests-2.32.4+computecanada scikit_learn-1.6.1+computecanada sentry-sdk-2.33.2+computecanada setproctitle-1.3.4+computecanada smmap-5.0.2+computecanada threadpoolctl-3.6.0+computecanada torch-2.7.1+computecanada tqdm-4.67.1+computecanada typing-inspection-0.4.1+computecanada urllib3-2.5.0+computecanada wandb-0.19.6+computecanada
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.6
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]Training:   0%|          | 0/1000 [00:01<?, ?epoch/s, Training loss (per batch)=0.11]Training:   0%|          | 1/1000 [00:01<17:55,  1.08s/epoch, Training loss (per batch)=0.11]Training:   0%|          | 1/1000 [00:01<17:55,  1.08s/epoch, Training loss (per batch)=0.07]Training:   0%|          | 2/1000 [00:01<12:57,  1.28epoch/s, Training loss (per batch)=0.07]Training:   0%|          | 2/1000 [00:02<12:57,  1.28epoch/s, Training loss (per batch)=0.07]Training:   0%|          | 3/1000 [00:02<11:14,  1.48epoch/s, Training loss (per batch)=0.07]Training:   0%|          | 3/1000 [00:02<11:14,  1.48epoch/s, Training loss (per batch)=0.08]Training:   0%|          | 4/1000 [00:02<10:32,  1.58epoch/s, Training loss (per batch)=0.08]Training:   0%|          | 4/1000 [00:03<10:32,  1.58epoch/s, Training loss (per batch)=0.12]Training:   0%|          | 5/1000 [00:03<10:11,  1.63epoch/s, Training loss (per batch)=0.12]Training:   0%|          | 5/1000 [00:03<10:11,  1.63epoch/s, Training loss (per batch)=0.16]Training:   1%|          | 6/1000 [00:03<09:54,  1.67epoch/s, Training loss (per batch)=0.16]Training:   1%|          | 6/1000 [00:04<09:54,  1.67epoch/s, Training loss (per batch)=0.19]Training:   1%|          | 7/1000 [00:04<09:40,  1.71epoch/s, Training loss (per batch)=0.19]Training:   1%|          | 7/1000 [00:05<09:40,  1.71epoch/s, Training loss (per batch)=0.21]Training:   1%|          | 8/1000 [00:05<09:39,  1.71epoch/s, Training loss (per batch)=0.21]Training:   1%|          | 8/1000 [00:05<09:39,  1.71epoch/s, Training loss (per batch)=0.33]Training:   1%|          | 9/1000 [00:05<09:31,  1.73epoch/s, Training loss (per batch)=0.33]Training:   1%|          | 9/1000 [00:06<09:31,  1.73epoch/s, Training loss (per batch)=0.42]Training:   1%|          | 10/1000 [00:06<09:25,  1.75epoch/s, Training loss (per batch)=0.42]Training:   1%|          | 10/1000 [00:06<09:25,  1.75epoch/s, Training loss (per batch)=0.39]Training:   1%|          | 11/1000 [00:06<09:22,  1.76epoch/s, Training loss (per batch)=0.39]Training:   1%|          | 11/1000 [00:07<09:22,  1.76epoch/s, Training loss (per batch)=0.63]Training:   1%|          | 12/1000 [00:07<09:15,  1.78epoch/s, Training loss (per batch)=0.63]Training:   1%|          | 12/1000 [00:07<09:15,  1.78epoch/s, Training loss (per batch)=0.22]Training:   1%|▏         | 13/1000 [00:07<09:15,  1.78epoch/s, Training loss (per batch)=0.22]Training:   1%|▏         | 13/1000 [00:08<09:15,  1.78epoch/s, Training loss (per batch)=0.05]Training:   1%|▏         | 14/1000 [00:08<09:10,  1.79epoch/s, Training loss (per batch)=0.05]Training:   1%|▏         | 14/1000 [00:08<09:10,  1.79epoch/s, Training loss (per batch)=0.04]Training:   2%|▏         | 15/1000 [00:08<09:08,  1.80epoch/s, Training loss (per batch)=0.04]Training:   2%|▏         | 15/1000 [00:09<09:08,  1.80epoch/s, Training loss (per batch)=0.05]Training:   2%|▏         | 16/1000 [00:09<09:06,  1.80epoch/s, Training loss (per batch)=0.05]Training:   2%|▏         | 16/1000 [00:10<09:06,  1.80epoch/s, Training loss (per batch)=0.07]Training:   2%|▏         | 17/1000 [00:10<09:05,  1.80epoch/s, Training loss (per batch)=0.07]Training:   2%|▏         | 17/1000 [00:10<09:05,  1.80epoch/s, Training loss (per batch)=0.09]Training:   2%|▏         | 18/1000 [00:10<09:03,  1.81epoch/s, Training loss (per batch)=0.09]Training:   2%|▏         | 18/1000 [00:11<09:03,  1.81epoch/s, Training loss (per batch)=0.09]Training:   2%|▏         | 19/1000 [00:11<09:05,  1.80epoch/s, Training loss (per batch)=0.09]Training:   2%|▏         | 19/1000 [00:11<09:05,  1.80epoch/s, Training loss (per batch)=0.10]Training:   2%|▏         | 20/1000 [00:11<09:04,  1.80epoch/s, Training loss (per batch)=0.10]Training:   2%|▏         | 20/1000 [00:12<09:04,  1.80epoch/s, Training loss (per batch)=0.10]Training:   2%|▏         | 21/1000 [00:12<09:05,  1.80epoch/s, Training loss (per batch)=0.10]Training:   2%|▏         | 21/1000 [00:12<09:05,  1.80epoch/s, Training loss (per batch)=0.31]Training:   2%|▏         | 22/1000 [00:12<09:05,  1.79epoch/s, Training loss (per batch)=0.31]Training:   2%|▏         | 22/1000 [00:13<09:05,  1.79epoch/s, Training loss (per batch)=0.30]Training:   2%|▏         | 23/1000 [00:13<09:03,  1.80epoch/s, Training loss (per batch)=0.30]Training:   2%|▏         | 23/1000 [00:13<09:03,  1.80epoch/s, Training loss (per batch)=0.14]Training:   2%|▏         | 24/1000 [00:13<09:03,  1.79epoch/s, Training loss (per batch)=0.14]Training:   2%|▏         | 24/1000 [00:14<09:03,  1.79epoch/s, Training loss (per batch)=0.03]Training:   2%|▎         | 25/1000 [00:14<09:03,  1.79epoch/s, Training loss (per batch)=0.03]Training:   2%|▎         | 25/1000 [00:15<09:03,  1.79epoch/s, Training loss (per batch)=0.03]Training:   3%|▎         | 26/1000 [00:15<09:00,  1.80epoch/s, Training loss (per batch)=0.03]Training:   3%|▎         | 26/1000 [00:15<09:00,  1.80epoch/s, Training loss (per batch)=0.03]Training:   3%|▎         | 27/1000 [00:15<09:00,  1.80epoch/s, Training loss (per batch)=0.03]Training:   3%|▎         | 27/1000 [00:16<09:00,  1.80epoch/s, Training loss (per batch)=0.06]Training:   3%|▎         | 28/1000 [00:16<08:59,  1.80epoch/s, Training loss (per batch)=0.06]Training:   3%|▎         | 28/1000 [00:16<08:59,  1.80epoch/s, Training loss (per batch)=0.15]Training:   3%|▎         | 29/1000 [00:16<08:58,  1.80epoch/s, Training loss (per batch)=0.15]Training:   3%|▎         | 29/1000 [00:17<08:58,  1.80epoch/s, Training loss (per batch)=0.06]Training:   3%|▎         | 30/1000 [00:17<08:59,  1.80epoch/s, Training loss (per batch)=0.06]Training:   3%|▎         | 30/1000 [00:17<08:59,  1.80epoch/s, Training loss (per batch)=0.03]Training:   3%|▎         | 31/1000 [00:17<08:59,  1.80epoch/s, Training loss (per batch)=0.03]Training:   3%|▎         | 31/1000 [00:18<08:59,  1.80epoch/s, Training loss (per batch)=0.03]Training:   3%|▎         | 32/1000 [00:18<08:58,  1.80epoch/s, Training loss (per batch)=0.03]Training:   3%|▎         | 32/1000 [00:18<08:58,  1.80epoch/s, Training loss (per batch)=0.03]Training:   3%|▎         | 33/1000 [00:18<08:58,  1.80epoch/s, Training loss (per batch)=0.03]Training:   3%|▎         | 33/1000 [00:19<08:58,  1.80epoch/s, Training loss (per batch)=0.03]Training:   3%|▎         | 34/1000 [00:19<08:56,  1.80epoch/s, Training loss (per batch)=0.03]Training:   3%|▎         | 34/1000 [00:20<08:56,  1.80epoch/s, Training loss (per batch)=0.03]Training:   4%|▎         | 35/1000 [00:20<08:54,  1.80epoch/s, Training loss (per batch)=0.03]Training:   4%|▎         | 35/1000 [00:20<08:54,  1.80epoch/s, Training loss (per batch)=0.03]Training:   4%|▎         | 36/1000 [00:20<08:52,  1.81epoch/s, Training loss (per batch)=0.03]Training:   4%|▎         | 36/1000 [00:21<08:52,  1.81epoch/s, Training loss (per batch)=0.03]Training:   4%|▎         | 37/1000 [00:21<08:51,  1.81epoch/s, Training loss (per batch)=0.03]Training:   4%|▎         | 37/1000 [00:21<08:51,  1.81epoch/s, Training loss (per batch)=0.03]Training:   4%|▍         | 38/1000 [00:21<08:49,  1.82epoch/s, Training loss (per batch)=0.03]Training:   4%|▍         | 38/1000 [00:22<08:49,  1.82epoch/s, Training loss (per batch)=0.03]Training:   4%|▍         | 39/1000 [00:22<08:50,  1.81epoch/s, Training loss (per batch)=0.03]Training:   4%|▍         | 39/1000 [00:22<08:50,  1.81epoch/s, Training loss (per batch)=0.03]Training:   4%|▍         | 40/1000 [00:22<08:52,  1.80epoch/s, Training loss (per batch)=0.03]Training:   4%|▍         | 40/1000 [00:23<08:52,  1.80epoch/s, Training loss (per batch)=0.03]Training:   4%|▍         | 41/1000 [00:23<08:50,  1.81epoch/s, Training loss (per batch)=0.03]Training:   4%|▍         | 41/1000 [00:23<08:50,  1.81epoch/s, Training loss (per batch)=0.05]Training:   4%|▍         | 42/1000 [00:23<08:48,  1.81epoch/s, Training loss (per batch)=0.05]Training:   4%|▍         | 42/1000 [00:24<08:48,  1.81epoch/s, Training loss (per batch)=0.09]Training:   4%|▍         | 43/1000 [00:24<08:48,  1.81epoch/s, Training loss (per batch)=0.09]Training:   4%|▍         | 43/1000 [00:25<08:48,  1.81epoch/s, Training loss (per batch)=0.09]Training:   4%|▍         | 44/1000 [00:25<08:47,  1.81epoch/s, Training loss (per batch)=0.09]Training:   4%|▍         | 44/1000 [00:25<08:47,  1.81epoch/s, Training loss (per batch)=0.07]Training:   4%|▍         | 45/1000 [00:25<08:46,  1.81epoch/s, Training loss (per batch)=0.07]Training:   4%|▍         | 45/1000 [00:26<08:46,  1.81epoch/s, Training loss (per batch)=0.05]Training:   5%|▍         | 46/1000 [00:26<08:46,  1.81epoch/s, Training loss (per batch)=0.05]Training:   5%|▍         | 46/1000 [00:26<08:46,  1.81epoch/s, Training loss (per batch)=0.03]Training:   5%|▍         | 47/1000 [00:26<08:43,  1.82epoch/s, Training loss (per batch)=0.03]Training:   5%|▍         | 47/1000 [00:27<08:43,  1.82epoch/s, Training loss (per batch)=0.02]Training:   5%|▍         | 48/1000 [00:27<08:48,  1.80epoch/s, Training loss (per batch)=0.02]Training:   5%|▍         | 48/1000 [00:27<08:48,  1.80epoch/s, Training loss (per batch)=0.02]Training:   5%|▍         | 49/1000 [00:27<08:46,  1.81epoch/s, Training loss (per batch)=0.02]Training:   5%|▍         | 49/1000 [00:28<08:46,  1.81epoch/s, Training loss (per batch)=0.04]Training:   5%|▌         | 50/1000 [00:28<08:44,  1.81epoch/s, Training loss (per batch)=0.04]Training:   5%|▌         | 50/1000 [00:28<08:44,  1.81epoch/s, Training loss (per batch)=0.06]Training:   5%|▌         | 51/1000 [00:28<08:50,  1.79epoch/s, Training loss (per batch)=0.06]Training:   5%|▌         | 51/1000 [00:29<08:50,  1.79epoch/s, Training loss (per batch)=0.07]Training:   5%|▌         | 52/1000 [00:29<08:49,  1.79epoch/s, Training loss (per batch)=0.07]Training:   5%|▌         | 52/1000 [00:30<08:49,  1.79epoch/s, Training loss (per batch)=0.06]Training:   5%|▌         | 53/1000 [00:30<08:51,  1.78epoch/s, Training loss (per batch)=0.06]Training:   5%|▌         | 53/1000 [00:30<08:51,  1.78epoch/s, Training loss (per batch)=0.04]Training:   5%|▌         | 54/1000 [00:30<08:45,  1.80epoch/s, Training loss (per batch)=0.04]Training:   5%|▌         | 54/1000 [00:31<08:45,  1.80epoch/s, Training loss (per batch)=0.04]Training:   6%|▌         | 55/1000 [00:31<08:46,  1.79epoch/s, Training loss (per batch)=0.04]Training:   6%|▌         | 55/1000 [00:31<08:46,  1.79epoch/s, Training loss (per batch)=0.04]Training:   6%|▌         | 56/1000 [00:31<08:43,  1.80epoch/s, Training loss (per batch)=0.04]Training:   6%|▌         | 56/1000 [00:32<08:43,  1.80epoch/s, Training loss (per batch)=0.04]Training:   6%|▌         | 57/1000 [00:32<08:41,  1.81epoch/s, Training loss (per batch)=0.04]Training:   6%|▌         | 57/1000 [00:32<08:41,  1.81epoch/s, Training loss (per batch)=0.04]Training:   6%|▌         | 58/1000 [00:32<08:39,  1.81epoch/s, Training loss (per batch)=0.04]Training:   6%|▌         | 58/1000 [00:33<08:39,  1.81epoch/s, Training loss (per batch)=0.04]Training:   6%|▌         | 59/1000 [00:33<08:38,  1.82epoch/s, Training loss (per batch)=0.04]Training:   6%|▌         | 59/1000 [00:33<08:38,  1.82epoch/s, Training loss (per batch)=0.04]Training:   6%|▌         | 60/1000 [00:33<08:40,  1.81epoch/s, Training loss (per batch)=0.04]Training:   6%|▌         | 60/1000 [00:34<08:40,  1.81epoch/s, Training loss (per batch)=0.03]Training:   6%|▌         | 61/1000 [00:34<08:42,  1.80epoch/s, Training loss (per batch)=0.03]Training:   6%|▌         | 61/1000 [00:35<08:42,  1.80epoch/s, Training loss (per batch)=0.02]Training:   6%|▌         | 62/1000 [00:35<08:40,  1.80epoch/s, Training loss (per batch)=0.02]Training:   6%|▌         | 62/1000 [00:35<08:40,  1.80epoch/s, Training loss (per batch)=0.02]Training:   6%|▋         | 63/1000 [00:35<08:42,  1.79epoch/s, Training loss (per batch)=0.02]Training:   6%|▋         | 63/1000 [00:36<08:42,  1.79epoch/s, Training loss (per batch)=0.02]Training:   6%|▋         | 64/1000 [00:36<08:41,  1.79epoch/s, Training loss (per batch)=0.02]Training:   6%|▋         | 64/1000 [00:36<08:41,  1.79epoch/s, Training loss (per batch)=0.03]Training:   6%|▋         | 65/1000 [00:36<08:41,  1.79epoch/s, Training loss (per batch)=0.03]Training:   6%|▋         | 65/1000 [00:37<08:41,  1.79epoch/s, Training loss (per batch)=0.03]Training:   7%|▋         | 66/1000 [00:37<08:39,  1.80epoch/s, Training loss (per batch)=0.03]Training:   7%|▋         | 66/1000 [00:37<08:39,  1.80epoch/s, Training loss (per batch)=0.02]Training:   7%|▋         | 67/1000 [00:37<08:37,  1.80epoch/s, Training loss (per batch)=0.02]Training:   7%|▋         | 67/1000 [00:38<08:37,  1.80epoch/s, Training loss (per batch)=0.02]Training:   7%|▋         | 68/1000 [00:38<08:33,  1.81epoch/s, Training loss (per batch)=0.02]Training:   7%|▋         | 68/1000 [00:38<08:33,  1.81epoch/s, Training loss (per batch)=0.02]Training:   7%|▋         | 69/1000 [00:38<08:34,  1.81epoch/s, Training loss (per batch)=0.02]Training:   7%|▋         | 69/1000 [00:39<08:34,  1.81epoch/s, Training loss (per batch)=0.02]Training:   7%|▋         | 70/1000 [00:39<08:32,  1.81epoch/s, Training loss (per batch)=0.02]Training:   7%|▋         | 70/1000 [00:39<08:32,  1.81epoch/s, Training loss (per batch)=0.02]Training:   7%|▋         | 71/1000 [00:39<08:30,  1.82epoch/s, Training loss (per batch)=0.02]Training:   7%|▋         | 71/1000 [00:40<08:30,  1.82epoch/s, Training loss (per batch)=0.01]Training:   7%|▋         | 72/1000 [00:40<08:28,  1.82epoch/s, Training loss (per batch)=0.01]Training:   7%|▋         | 72/1000 [00:41<08:28,  1.82epoch/s, Training loss (per batch)=0.01]Training:   7%|▋         | 73/1000 [00:41<08:28,  1.82epoch/s, Training loss (per batch)=0.01]Training:   7%|▋         | 73/1000 [00:41<08:28,  1.82epoch/s, Training loss (per batch)=0.01]Training:   7%|▋         | 74/1000 [00:41<08:28,  1.82epoch/s, Training loss (per batch)=0.01]Training:   7%|▋         | 74/1000 [00:42<08:28,  1.82epoch/s, Training loss (per batch)=0.01]Training:   8%|▊         | 75/1000 [00:42<08:31,  1.81epoch/s, Training loss (per batch)=0.01]Training:   8%|▊         | 75/1000 [00:42<08:31,  1.81epoch/s, Training loss (per batch)=0.01]Training:   8%|▊         | 76/1000 [00:42<08:29,  1.81epoch/s, Training loss (per batch)=0.01]Training:   8%|▊         | 76/1000 [00:43<08:29,  1.81epoch/s, Training loss (per batch)=0.01]Training:   8%|▊         | 77/1000 [00:43<08:28,  1.82epoch/s, Training loss (per batch)=0.01]Training:   8%|▊         | 77/1000 [00:43<08:28,  1.82epoch/s, Training loss (per batch)=0.01]Training:   8%|▊         | 78/1000 [00:43<08:27,  1.82epoch/s, Training loss (per batch)=0.01]Training:   8%|▊         | 78/1000 [00:44<08:27,  1.82epoch/s, Training loss (per batch)=0.01]Training:   8%|▊         | 79/1000 [00:44<08:25,  1.82epoch/s, Training loss (per batch)=0.01]Training:   8%|▊         | 79/1000 [00:44<08:25,  1.82epoch/s, Training loss (per batch)=0.01]Training:   8%|▊         | 80/1000 [00:44<08:29,  1.80epoch/s, Training loss (per batch)=0.01]Training:   8%|▊         | 80/1000 [00:45<08:29,  1.80epoch/s, Training loss (per batch)=0.01]Training:   8%|▊         | 81/1000 [00:45<08:25,  1.82epoch/s, Training loss (per batch)=0.01]Training:   8%|▊         | 81/1000 [00:46<08:25,  1.82epoch/s, Training loss (per batch)=0.02]Training:   8%|▊         | 82/1000 [00:46<08:26,  1.81epoch/s, Training loss (per batch)=0.02]Training:   8%|▊         | 82/1000 [00:46<08:26,  1.81epoch/s, Training loss (per batch)=0.02]Training:   8%|▊         | 83/1000 [00:46<08:25,  1.81epoch/s, Training loss (per batch)=0.02]Training:   8%|▊         | 83/1000 [00:47<08:25,  1.81epoch/s, Training loss (per batch)=0.02]Training:   8%|▊         | 84/1000 [00:47<08:23,  1.82epoch/s, Training loss (per batch)=0.02]Training:   8%|▊         | 84/1000 [00:47<08:23,  1.82epoch/s, Training loss (per batch)=0.02]Training:   8%|▊         | 85/1000 [00:47<08:24,  1.81epoch/s, Training loss (per batch)=0.02]Training:   8%|▊         | 85/1000 [00:48<08:24,  1.81epoch/s, Training loss (per batch)=0.02]Training:   9%|▊         | 86/1000 [00:48<08:22,  1.82epoch/s, Training loss (per batch)=0.02]Training:   9%|▊         | 86/1000 [00:48<08:22,  1.82epoch/s, Training loss (per batch)=0.01]Training:   9%|▊         | 87/1000 [00:48<08:22,  1.82epoch/s, Training loss (per batch)=0.01]Training:   9%|▊         | 87/1000 [00:49<08:22,  1.82epoch/s, Training loss (per batch)=0.01]Training:   9%|▉         | 88/1000 [00:49<08:21,  1.82epoch/s, Training loss (per batch)=0.01]Training:   9%|▉         | 88/1000 [00:49<08:21,  1.82epoch/s, Training loss (per batch)=0.01]Training:   9%|▉         | 89/1000 [00:49<08:20,  1.82epoch/s, Training loss (per batch)=0.01]slurmstepd: error: *** JOB 47178032 ON ng31203 CANCELLED AT 2025-07-30T14:36:31 ***
